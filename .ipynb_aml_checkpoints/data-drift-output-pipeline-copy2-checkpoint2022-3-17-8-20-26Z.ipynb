{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "#\n",
        "\n",
        "src_folder = 'steps'\n",
        "cluster_name = 'cpu-cluster'\n",
        "env_name = 'data-drift-env'\n",
        "\n",
        "DATASTORE_NAME = 'workspaceblobstore'\n",
        "FILE_DATASET_NAME = 'datadrift_file_results'\n",
        "json_file_path = f'datadrift/metrics/**/output_*.json'\n",
        "\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to work with', ws.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to work with evolve-ml\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1650175017621
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # what is the purpose of this script:\n",
        " pipeline which is schudles daily(?) \n",
        "\n",
        " collect data drift detector output, transform\n",
        " \n",
        " result: dataset that can be used\n",
        "\n",
        "```json\n",
        "\n",
        "[{\"runid\": \"mslearn-diabates-drift-scheedule-Monitor-Runs_1649806360074\", \"drift_threshold\": 0.3, \"pipeline_starttime\": \"2022-04-12T23:34:36.163569Z\", \"run_type\": \"Scheduler\", \"datadrift_id\": \"0eb10f47-1629-4014-9db8-680eb438328c\", \"datadrift_name\": \"mslearn-diabates-drift-scheedule\", \"datadrift_configuration_type\": \"DatasetBased\", \"start_date\": \"2022-04-11\", \"end_date\": \"2022-04-12\", \"baseline_dataset_id\": \"8ab2e553-c4d6-4ee5-ac83-574ef324496b\", \"target_dataset_id\": \"ce614e9a-70d5-4ab0-949a-18cf1ee9b991\", \"frequency\": \"Day\", \"from_dataset\": \"both\", \"column_name\": \"Pregnancies\", \"metric_category\": \"statistical_distance\", \"metric_type\": \"column\", \"column_type\": \"numerical\", \"name\": \"wasserstein_distance\", \"value\": 5.950910447009183}... ]\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $src_folder/utils.py\n",
        "\n",
        "from contextlib import contextmanager\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def temp_directory(dir_name = 'temp', **kwds):\n",
        "    \n",
        "    os.makedirs(dir_name, exist_ok=True)\n",
        "    \n",
        "    try:\n",
        "        yield dir_name\n",
        "\n",
        "    except Exception:\n",
        "        print(f\"Unable to create '{dir_name}'\")\n",
        "\n",
        "    finally:\n",
        "        shutil.rmtree(dir_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing steps/utils.py\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $src_folder/transform-data-drift-output.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import bigjson\n",
        "import os\n",
        "import utils\n",
        "from azureml.core import Workspace, Dataset, Datastore, Run\n",
        "from azureml.core.run import _OfflineRun\n",
        "\n",
        "\n",
        "TEMP_DIRECTORY = 'temp'\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\n",
        "    parser.add_argument('--transformed-data', type=str, dest='transformed_data', default='transformed_data', help='Folder for results')\n",
        "    return parser.parse_args()\n",
        "\n",
        "args = parse_args()\n",
        "print(f'Arguments: {args.__dict__}')\n",
        "save_folder = args.transformed_data\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# access the ws either wehn running offline or remote run \n",
        "ws = Workspace.from_config() if type(run) == _OfflineRun else run.experiment.workspace\n",
        "dstore = Datastore.get_default(ws)\n",
        "\n",
        "file_dataset = run.input_datasets['raw_data']\n",
        "\n",
        "print(file_dataset)\n",
        "\n",
        "with utils.temp_directory(TEMP_DIRECTORY):\n",
        "    # Download json files defined by the dataset to temp directory\n",
        "    json_file_paths = file_dataset.download(f'{TEMP_DIRECTORY}', overwrite=True)\n",
        "\n",
        "    # Convert json files to jsonl files (in local directory) \n",
        "    for json_path in json_file_paths:\n",
        "        \n",
        "        # Read json file in streaming mode\n",
        "        with open(json_path, 'rb') as f:\n",
        "            json_data = bigjson.load(f)\n",
        "            # Replace file name extension\n",
        "            jsonl_path = os.path.splitext(json_path)[0]+'.jsonl'\n",
        "\n",
        "            # Open jsonl file  \n",
        "            with open(jsonl_path, 'w') as jsonl_file:\n",
        "                # Iterates over input json\n",
        "                for data in json_data:\n",
        "                    # Converts json to a Python dict  \n",
        "                    dict_data = data.to_python()\n",
        "                    \n",
        "                    # Saves the data to jsonl file\n",
        "                    jsonl_file.write(json.dumps(dict_data)+\"\\n\")\n",
        "                    \n",
        "        # Delete json file\n",
        "        os.remove(json_path)\n",
        "\n",
        "    # Upload jsonl files to datastore\n",
        "    print(\"Saving Transformed Data...\")\n",
        "    print(os.listdir(f'{TEMP_DIRECTORY}'))\n",
        "    output_dataset = Dataset.File.upload_directory(f'{TEMP_DIRECTORY}', target=save_folder)\n",
        "    print(os.listdir(f'{save_folder}'))\n",
        "\n",
        "print(os.listdir(f'{save_folder}'))\n",
        "\n",
        "#TODO: \n",
        "## move to util\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting steps/transform-data-drift-output.py\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $src_folder/save-data-drift-output.py\n",
        "\n",
        "import argparse\n",
        "from azureml.core import Dataset, Datastore, Run\n",
        "from azureml.core.run import _OfflineRun\n",
        "from azureml.data.dataset_factory import DataType\n",
        "\n",
        "PARTITION_FORMAT = '{DATADRIFT_ID}/{PARTITION_DATE:yyyy/MM/dd}/output_{RUN_ID}.json'\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--transformed-data\", type=str, dest='transformed_data', help='transformed data')\n",
        "    return parser.parse_args()\n",
        "\n",
        "args = parse_args()\n",
        "print(f'Arguments: {args.__dict__}')\n",
        "transformed_data = args.transformed_data\n",
        "print(transformed_data)\n",
        "\n",
        "\n",
        "# Crate TabularDataSet based on converted jsonl files\n",
        "output_dataset = Dataset.Tabular.from_json_lines_files(path=transformed_data, partition_format=PARTITION_FORMAT)\n",
        "output_dataset = output_dataset.register_on_complete(name='datadrift_results_pipeline', description = 'datadrift results pipeline')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting steps/save-data-drift-output.py\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    from azureml.core.environment import Environment\n",
        "    from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "    myenv = Environment(name=env_name)\n",
        "    conda_dep = CondaDependencies()\n",
        "\n",
        "    conda_dep.add_pip_package('bigjson')\n",
        "    conda_dep.add_pip_package('azureml-defaults')\n",
        "\n",
        "    # Adds dependencies to PythonSection of myenv\n",
        "    myenv.python.conda_dependencies=conda_dep\n",
        "\n",
        "    myenv.register(ws).build(ws)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649873271621
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import RunConfiguration, ComputeTarget, Environment\n",
        "\n",
        "run_config = RunConfiguration()\n",
        "run_config.environment = Environment.get(ws, env_name)\n",
        "compute_target = ComputeTarget(ws, cluster_name)\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1650175054781
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineData, PipelineParameter\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.core import ComputeTarget, Datastore, Dataset\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "\n",
        "#datadir_param = PipelineData('datadir', is_directory=True)\n",
        "#evolve_param = PipelineData('evolve')\n",
        "#azure_param = PipelineData('azure')\n",
        "#compare_param = PipelineData('compare')\n",
        "\n",
        "#collection_param = PipelineParameter(name=\"collection\", default_value='test_datasets')\n",
        "#repo_param = PipelineParameter(name=\"repo\", default_value='test')\n",
        "\n",
        "\n",
        "collection_param = PipelineParameter(name=\"collection\", default_value='test_datasets')\n",
        "repo_param = PipelineParameter(name=\"repo\", default_value='test')\n",
        "\n",
        "\n",
        "# Get data-drift output dataset\n",
        "dstore = Datastore.get(ws, DATASTORE_NAME)\n",
        "metrics_ds = Dataset.File.from_files(path=(dstore,json_file_path)) # add filter dataset\n",
        "\n",
        "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\n",
        "transformed_data = OutputFileDatasetConfig(\"transformed_data\")\n",
        "\n",
        "# step 0, collate the data drift outputs of dstore \n",
        "## use the path-pattern to load files \n",
        "## filter(-1 month)\n",
        "\n",
        "# Step 1, data transofrm (from json to jsonl), save to dstore as .jsonl files\n",
        "## future todo: Parallel-ize the json files transformation  \n",
        "transform_step = PythonScriptStep(\n",
        "    name='transform data drift output',\n",
        "    source_directory=src_folder,\n",
        "    script_name='transform-data-drift-output.py',\n",
        "    arguments = ['--input-data', metrics_ds.as_named_input('raw_data'),\n",
        "                '--transformed-data', transformed_data],                \n",
        "    compute_target=compute_target, \n",
        "    runconfig=run_config, \n",
        "    allow_reuse=False,    \n",
        ")\n",
        "\n",
        "# Step 2, collate the .jsonl file, partition accordingly (time-series), register new version per Output Dataset (Tabular) \n",
        "## future - iron out the duplicated / contradictory data points \n",
        "save_step = PythonScriptStep(\n",
        "    name='save data drift output',\n",
        "    source_directory=src_folder,\n",
        "    script_name='save-data-drift-output.py',\n",
        "    arguments = ['--transformed-data', transformed_data.as_input()],\n",
        "    compute_target=compute_target, \n",
        "    runconfig=run_config,    \n",
        "    allow_reuse=False,\n",
        ")\n",
        "\n",
        "print(\"Pipeline steps defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline steps defined\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1650175069319
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# Construct the pipeline\n",
        "pipeline_steps = [transform_step, save_step]\n",
        "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
        "print(\"Pipeline is built.\")\n",
        "\n",
        "# Create an experiment and run the pipeline\n",
        "experiment = Experiment(workspace=ws, name = 'data-drift-output-exeriment')\n",
        "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True) \n",
        "print(\"Pipeline submitted for execution.\")\n",
        "\n",
        "RunDetails(pipeline_run).show()\n",
        "pipeline_run.wait_for_completion(show_output=True)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built.\nCreated step transform data drift output [35f82fb6][ce0e88fe-c9da-4a89-a050-ff4599488974], (This step will run and generate new outputs)\nCreated step save data drift output [751e847b][e69bc83a-b74c-42d3-84b4-4c84dfe0c8d0], (This step will run and generate new outputs)\nSubmitted PipelineRun d979f672-c1bd-4f3f-ac0c-8a180e6c831d\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/d979f672-c1bd-4f3f-ac0c-8a180e6c831d?wsid=/subscriptions/06c3d5bb-46a2-4d92-9508-c018d06f6452/resourcegroups/evolve-team-rg/workspaces/evolve-ml&tid=72a43063-967e-43c8-8121-0823266b2701\nPipeline submitted for execution.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f6a35e4ee0c43dbaa7b461c23f98927"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/d979f672-c1bd-4f3f-ac0c-8a180e6c831d?wsid=/subscriptions/06c3d5bb-46a2-4d92-9508-c018d06f6452/resourcegroups/evolve-team-rg/workspaces/evolve-ml&tid=72a43063-967e-43c8-8121-0823266b2701\", \"run_id\": \"d979f672-c1bd-4f3f-ac0c-8a180e6c831d\", \"run_properties\": {\"run_id\": \"d979f672-c1bd-4f3f-ac0c-8a180e6c831d\", \"created_utc\": \"2022-04-17T06:33:45.129997Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\", \"azureml.continue_on_step_failure\": \"False\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"tags\": {}, \"end_time_utc\": \"2022-04-17T06:34:47.642293Z\", \"status\": \"Failed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.d979f672-c1bd-4f3f-ac0c-8a180e6c831d/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=oquyRFp2jgiSto%2B8Cc2aO9%2FK8%2Ftftztzi0zhQZe5p%2Bs%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T07%3A39%3A03Z&se=2022-04-17T15%3A49%3A03Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.d979f672-c1bd-4f3f-ac0c-8a180e6c831d/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=VZ9c%2FQTZ6Hto8FUtgzM%2Bm%2FBFPUFH5DpIESXc6FNDeYM%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T07%3A39%3A03Z&se=2022-04-17T15%3A49%3A03Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.d979f672-c1bd-4f3f-ac0c-8a180e6c831d/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=goM9L9gvBGX6wJLCmXKZ7ytJtrtPCSJI2WiXZjDdris%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T07%3A39%3A03Z&se=2022-04-17T15%3A49%3A03Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:01:02\", \"run_number\": \"1650177225\", \"run_queued_details\": {\"status\": \"Failed\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"708ccaca-90d9-4a93-bb48-eadcab1cc401\", \"name\": \"transform data drift output\", \"status\": \"Finished\", \"start_time\": \"2022-04-17T06:33:57.756798Z\", \"created_time\": \"2022-04-17T06:33:47.836347Z\", \"end_time\": \"2022-04-17T06:34:19.48773Z\", \"duration\": \"0:00:31\", \"run_number\": 1650177227, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2022-04-17T06:33:47.836347Z\", \"is_reused\": \"\"}, {\"run_id\": \"93a69cb2-ab1e-42b6-b89a-c75a0a4add19\", \"name\": \"save data drift output\", \"status\": \"Failed\", \"start_time\": \"2022-04-17T06:34:27.817885Z\", \"created_time\": \"2022-04-17T06:34:21.893502Z\", \"end_time\": \"2022-04-17T06:34:45.854025Z\", \"duration\": \"0:00:23\", \"run_number\": 1650177261, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2022-04-17T06:34:21.893502Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2022-04-17 06:33:47Z] Submitting 1 runs, first five are: 35f82fb6:708ccaca-90d9-4a93-bb48-eadcab1cc401\\n[2022-04-17 06:34:21Z] Completing processing run id 708ccaca-90d9-4a93-bb48-eadcab1cc401.\\n[2022-04-17 06:34:21Z] Submitting 1 runs, first five are: 751e847b:93a69cb2-ab1e-42b6-b89a-c75a0a4add19\\n[2022-04-17 06:34:47Z] Execution of experiment failed, update experiment status and cancel running nodes.\\n\", \"graph\": {\"datasource_nodes\": {\"56bd59ee\": {\"node_id\": \"56bd59ee\", \"name\": \"datadrift_file_results\"}}, \"module_nodes\": {\"35f82fb6\": {\"node_id\": \"35f82fb6\", \"name\": \"transform data drift output\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"708ccaca-90d9-4a93-bb48-eadcab1cc401\"}, \"751e847b\": {\"node_id\": \"751e847b\", \"name\": \"save data drift output\", \"status\": \"Failed\", \"_is_reused\": false, \"run_id\": \"93a69cb2-ab1e-42b6-b89a-c75a0a4add19\"}}, \"edges\": [{\"source_node_id\": \"56bd59ee\", \"source_node_name\": \"datadrift_file_results\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"35f82fb6\", \"dst_node_name\": \"transform data drift output\"}, {\"source_node_id\": \"35f82fb6\", \"source_node_name\": \"transform data drift output\", \"source_name\": \"transformed_data\", \"target_name\": \"input_7c0bc4da\", \"dst_node_id\": \"751e847b\", \"dst_node_name\": \"save data drift output\"}], \"child_runs\": [{\"run_id\": \"708ccaca-90d9-4a93-bb48-eadcab1cc401\", \"name\": \"transform data drift output\", \"status\": \"Finished\", \"start_time\": \"2022-04-17T06:33:57.756798Z\", \"created_time\": \"2022-04-17T06:33:47.836347Z\", \"end_time\": \"2022-04-17T06:34:19.48773Z\", \"duration\": \"0:00:31\", \"run_number\": 1650177227, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2022-04-17T06:33:47.836347Z\", \"is_reused\": \"\"}, {\"run_id\": \"93a69cb2-ab1e-42b6-b89a-c75a0a4add19\", \"name\": \"save data drift output\", \"status\": \"Failed\", \"start_time\": \"2022-04-17T06:34:27.817885Z\", \"created_time\": \"2022-04-17T06:34:21.893502Z\", \"end_time\": \"2022-04-17T06:34:45.854025Z\", \"duration\": \"0:00:23\", \"run_number\": 1650177261, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2022-04-17T06:34:21.893502Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.39.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PipelineRunId: d979f672-c1bd-4f3f-ac0c-8a180e6c831d\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/d979f672-c1bd-4f3f-ac0c-8a180e6c831d?wsid=/subscriptions/06c3d5bb-46a2-4d92-9508-c018d06f6452/resourcegroups/evolve-team-rg/workspaces/evolve-ml&tid=72a43063-967e-43c8-8121-0823266b2701\nPipelineRun Status: NotStarted\nPipelineRun Status: Running\n\n\nStepRunId: 708ccaca-90d9-4a93-bb48-eadcab1cc401\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/708ccaca-90d9-4a93-bb48-eadcab1cc401?wsid=/subscriptions/06c3d5bb-46a2-4d92-9508-c018d06f6452/resourcegroups/evolve-team-rg/workspaces/evolve-ml&tid=72a43063-967e-43c8-8121-0823266b2701\nStepRun( transform data drift output ) Status: Running\n\nStepRun(transform data drift output) Execution Summary\n=======================================================\nStepRun( transform data drift output ) Status: Finished\n{'runId': '708ccaca-90d9-4a93-bb48-eadcab1cc401', 'target': 'cpu-cluster', 'status': 'Completed', 'startTimeUtc': '2022-04-17T06:33:57.756798Z', 'endTimeUtc': '2022-04-17T06:34:19.48773Z', 'services': {}, 'properties': {'ContentSnapshotId': 'e4079bc2-64c5-43b4-b66c-9758238e3709', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'ce0e88fe-c9da-4a89-a050-ff4599488974', 'azureml.moduleName': 'transform data drift output', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '35f82fb6', 'azureml.pipelinerunid': 'd979f672-c1bd-4f3f-ac0c-8a180e6c831d', 'azureml.pipeline': 'd979f672-c1bd-4f3f-ac0c-8a180e6c831d', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlctrain', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '58ddad62-e26a-4f7b-bcce-c6a15afdc60e'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'raw_data', 'mechanism': 'Direct'}}], 'outputDatasets': [{'identifier': {'savedId': '66a67955-3984-407b-8223-42950f297457'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'transformed_data'}, 'dataset': {\n  \"source\": [\n    \"('workspaceblobstore', 'dataset/708ccaca-90d9-4a93-bb48-eadcab1cc401/transformed_data/')\"\n  ],\n  \"definition\": [\n    \"GetDatastoreFiles\"\n  ],\n  \"registration\": {\n    \"id\": \"66a67955-3984-407b-8223-42950f297457\",\n    \"name\": null,\n    \"version\": null,\n    \"workspace\": \"Workspace.create(name='evolve-ml', subscription_id='06c3d5bb-46a2-4d92-9508-c018d06f6452', resource_group='evolve-team-rg')\"\n  }\n}}], 'runDefinition': {'script': 'transform-data-drift-output.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--input-data', 'DatasetConsumptionConfig:raw_data', '--transformed-data', 'DatasetOutputConfig:transformed_data'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'cpu-cluster', 'dataReferences': {}, 'data': {'raw_data': {'dataLocation': {'dataset': {'id': '58ddad62-e26a-4f7b-bcce-c6a15afdc60e', 'name': None, 'version': None}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'raw_data', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {'transformed_data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'workspaceblobstore', 'relativePath': None}, 'uri': None, 'type': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': None, 'description': None, 'tags': None, 'properties': {'azureml.pipelineRunId': 'd979f672-c1bd-4f3f-ac0c-8a180e6c831d', 'azureml.pipelineRun.moduleNodeId': '35f82fb6', 'azureml.pipelineRun.outputPortName': 'transformed_data'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'data-drift-env', 'version': '1', 'assetId': 'azureml://locations/westeurope/workspaces/899eb723-9471-475e-9ea2-abcad8a41de5/environments/data-drift-env/versions/1', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['bigjson', 'azureml-defaults']}], 'name': 'azureml_d9438b93de534f7f3a68847348170eaf'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/backgroundProcess.log': 'https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.708ccaca-90d9-4a93-bb48-eadcab1cc401/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=BNHFT4889t9vt0mC%2FUOmbzcpdo%2Bg8%2FChhcvT82ubN%2B4%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T06%3A24%3A16Z&se=2022-04-17T14%3A34%3A16Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.708ccaca-90d9-4a93-bb48-eadcab1cc401/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=jbPZRgsx1r%2FPf1ttorvB63AKib7y2lwFULfsKgCabeQ%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T06%3A24%3A16Z&se=2022-04-17T14%3A34%3A16Z&sp=r', 'logs/azureml/dataprep/rslex.log': 'https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.708ccaca-90d9-4a93-bb48-eadcab1cc401/logs/azureml/dataprep/rslex.log?sv=2019-07-07&sr=b&sig=msoZP5FgsdKf0pAZVAajG14iFBTtbVZDZOG5NwyJgg0%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T06%3A24%3A16Z&se=2022-04-17T14%3A34%3A16Z&sp=r', 'logs/azureml/dataprep/rslex.log.2022-04-17-06': 'https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.708ccaca-90d9-4a93-bb48-eadcab1cc401/logs/azureml/dataprep/rslex.log.2022-04-17-06?sv=2019-07-07&sr=b&sig=Y9KZ2vH%2FLT1UmZZTE4GDKIwZOekoMoHaC3MydDB2dzM%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T06%3A24%3A16Z&se=2022-04-17T14%3A34%3A16Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.708ccaca-90d9-4a93-bb48-eadcab1cc401/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=uDJM6FIxai7tcZU4B%2BOS9h6GcglIKP%2FGV7FFsyT130c%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T06%3A24%3A16Z&se=2022-04-17T14%3A34%3A16Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.708ccaca-90d9-4a93-bb48-eadcab1cc401/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=XEyfEI6wF2RZXRYsvOAHoGkXAAdQAV40rgukAPOZQ30%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T06%3A24%3A16Z&se=2022-04-17T14%3A34%3A16Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://evolveml3040211544.blob.core.windows.net/azureml/ExperimentRun/dcid.708ccaca-90d9-4a93-bb48-eadcab1cc401/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=9S8i%2B9tHCEqe15i9DrbHWN8m3ZcUkI2z5biQ0WyGt4o%3D&skoid=a6efb8ac-b3a0-40a4-8779-354b593f2dab&sktid=72a43063-967e-43c8-8121-0823266b2701&skt=2022-04-16T23%3A13%3A51Z&ske=2022-04-18T07%3A23%3A51Z&sks=b&skv=2019-07-07&st=2022-04-17T06%3A24%3A16Z&se=2022-04-17T14%3A34%3A16Z&sp=r'}, 'submittedBy': 'hadar benin'}\n\n\n\n\nStepRunId: 93a69cb2-ab1e-42b6-b89a-c75a0a4add19\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/93a69cb2-ab1e-42b6-b89a-c75a0a4add19?wsid=/subscriptions/06c3d5bb-46a2-4d92-9508-c018d06f6452/resourcegroups/evolve-team-rg/workspaces/evolve-ml&tid=72a43063-967e-43c8-8121-0823266b2701\nStepRun( save data drift output ) Status: Running\n\nStepRun(save data drift output) Execution Summary\n==================================================\nStepRun( save data drift output ) Status: Failed\n\nWarnings:\n{\n  \"error\": {\n    \"code\": \"UserError\",\n    \"severity\": null,\n    \"message\": \"AzureMLCompute job failed.\\ndata-capability.DatasetMountSession:input_7c0bc4da.ExecutionError: \\nError Code: ScriptExecution.StreamAccess.NotFound\\n\\n\\tReason: [REDACTED]\\n\\tStackTrace:   File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data\",\n    \"messageFormat\": \"{Message}\",\n    \"messageParameters\": {\n      \"Message\": \"AzureMLCompute job failed.\\ndata-capability.DatasetMountSession:input_7c0bc4da.ExecutionError: \\nError Code: ScriptExecution.StreamAccess.NotFound\\n\\n\\tReason: [REDACTED]\\n\\tStackTrace:   File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/capability_session.py\\\", line 47, in start\\n    (data_path, sub_data_path) = session.start()\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\", line 182, in start\\n    if self._is_single_file:\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\", line 130, in _is_single_file\\n    path = dataflow._to_pyrecords()[0][temp_column]\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/dataflow.py\\\", line 758, in _to_pyrecords\\n    intermediate_files = _write_preppy_with_fallback('Dataflow.to_pyrecords', self, span_context=to_dprep_span_context(span.get_context()))\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 192, in _write_preppy_with_fallback\\n    _execute_with_fallback(activity, dataflow_to_execute, force_clex=force_clex, span_context=span_context)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 240, in _execute_with_fallback\\n    clex_execute()\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 221, in clex_execute\\n    span_context=span_context\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_aml_helper.py\\\", line 38, in wrapper\\n    return send_message_func(op_code, message, cancellation_token)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/api.py\\\", line 159, in execute_anonymous_activity\\n    response = self._message_channel.send_message('Engine.ExecuteActivity', message_args, cancellation_token)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/engine.py\\\", line 291, in send_message\\n    raise_engine_error(response['error'])\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/errorhandlers.py\\\", line 10, in raise_engine_error\\n    raise ExecutionError(error_response)\\n\"\n    },\n    \"referenceCode\": null,\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": {\n      \"code\": \"UserTrainingScriptFailed\",\n      \"innerError\": null\n    },\n    \"debugInfo\": null,\n    \"additionalInfo\": null\n  },\n  \"correlation\": {\n    \"operation\": \"de2ec978135f506d50e2e1a40cfa58cb\",\n    \"request\": \"fe39c0364112310f\"\n  },\n  \"environment\": \"westeurope\",\n  \"location\": \"westeurope\",\n  \"time\": \"2022-04-17T06:34:45.7791801+00:00\",\n  \"componentName\": \"globaljobdispatcher\"\n}\n"
        },
        {
          "output_type": "error",
          "ename": "ActivityFailedException",
          "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"{'code': data-capability.DatasetMountSession:input_7c0bc4da.ExecutionError, 'message': \\nError Code: ScriptExecution.StreamAccess.NotFound\\n, 'target': , 'category': UserError, 'error_details': [{'key': NonCompliantReason, 'value': \\nError Code: ScriptExecution.StreamAccess.NotFound\\nFailed Step: 6e7dc11a-b948-4ac1-b654-2fbdf1a594e6\\nError Message: ScriptExecutionException was caused by StreamAccessException.\\n  StreamAccessException was caused by NotFoundException.\\n    Found no resources for the input provided: 'https://evolveml3040211544.blob.core.windows.net/azureml-blobstore-899eb723-9471-475e-9ea2-abcad8a41de5/dataset/708ccaca-90d9-4a93-bb48-eadcab1cc401/transformed_data/'\\n| session_id=1d79f0f3-0532-417f-95b5-11e7caadde3f}, {'key': StackTrace, 'value':   File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/capability_session.py\\\", line 47, in start\\n    (data_path, sub_data_path) = session.start()\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\", line 182, in start\\n    if self._is_single_file:\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\", line 130, in _is_single_file\\n    path = dataflow._to_pyrecords()[0][temp_column]\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/dataflow.py\\\", line 758, in _to_pyrecords\\n    intermediate_files = _write_preppy_with_fallback('Dataflow.to_pyrecords', self, span_context=to_dprep_span_context(span.get_context()))\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 192, in _write_preppy_with_fallback\\n    _execute_with_fallback(activity, dataflow_to_execute, force_clex=force_clex, span_context=span_context)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 240, in _execute_with_fallback\\n    clex_execute()\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 221, in clex_execute\\n    span_context=span_context\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_aml_helper.py\\\", line 38, in wrapper\\n    return send_message_func(op_code, message, cancellation_token)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/api.py\\\", line 159, in execute_anonymous_activity\\n    response = self._message_channel.send_message('Engine.ExecuteActivity', message_args, cancellation_token)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/engine.py\\\", line 291, in send_message\\n    raise_engine_error(response['error'])\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/errorhandlers.py\\\", line 10, in raise_engine_error\\n    raise ExecutionError(error_response)\\n}\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"{'code': data-capability.DatasetMountSession:input_7c0bc4da.ExecutionError, 'message': \\\\nError Code: ScriptExecution.StreamAccess.NotFound\\\\n, 'target': , 'category': UserError, 'error_details': [{'key': NonCompliantReason, 'value': \\\\nError Code: ScriptExecution.StreamAccess.NotFound\\\\nFailed Step: 6e7dc11a-b948-4ac1-b654-2fbdf1a594e6\\\\nError Message: ScriptExecutionException was caused by StreamAccessException.\\\\n  StreamAccessException was caused by NotFoundException.\\\\n    Found no resources for the input provided: 'https://evolveml3040211544.blob.core.windows.net/azureml-blobstore-899eb723-9471-475e-9ea2-abcad8a41de5/dataset/708ccaca-90d9-4a93-bb48-eadcab1cc401/transformed_data/'\\\\n| session_id=1d79f0f3-0532-417f-95b5-11e7caadde3f}, {'key': StackTrace, 'value':   File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/capability_session.py\\\\\\\", line 47, in start\\\\n    (data_path, sub_data_path) = session.start()\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\\\\\", line 182, in start\\\\n    if self._is_single_file:\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\\\\\", line 130, in _is_single_file\\\\n    path = dataflow._to_pyrecords()[0][temp_column]\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/dataflow.py\\\\\\\", line 758, in _to_pyrecords\\\\n    intermediate_files = _write_preppy_with_fallback('Dataflow.to_pyrecords', self, span_context=to_dprep_span_context(span.get_context()))\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\\\\\", line 192, in _write_preppy_with_fallback\\\\n    _execute_with_fallback(activity, dataflow_to_execute, force_clex=force_clex, span_context=span_context)\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\\\\\", line 240, in _execute_with_fallback\\\\n    clex_execute()\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\\\\\", line 221, in clex_execute\\\\n    span_context=span_context\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_aml_helper.py\\\\\\\", line 38, in wrapper\\\\n    return send_message_func(op_code, message, cancellation_token)\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/api.py\\\\\\\", line 159, in execute_anonymous_activity\\\\n    response = self._message_channel.send_message('Engine.ExecuteActivity', message_args, cancellation_token)\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/engine.py\\\\\\\", line 291, in send_message\\\\n    raise_engine_error(response['error'])\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/errorhandlers.py\\\\\\\", line 10, in raise_engine_error\\\\n    raise ExecutionError(error_response)\\\\n}\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e5101c64596f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpipeline_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    293\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                                 step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n\u001b[0m\u001b[1;32m    296\u001b[0m                                                              raise_on_error=raise_on_error)\n\u001b[1;32m    297\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n\u001b[0m\u001b[1;32m    738\u001b[0m                                                raise_on_error=raise_on_error)\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"{'code': data-capability.DatasetMountSession:input_7c0bc4da.ExecutionError, 'message': \\nError Code: ScriptExecution.StreamAccess.NotFound\\n, 'target': , 'category': UserError, 'error_details': [{'key': NonCompliantReason, 'value': \\nError Code: ScriptExecution.StreamAccess.NotFound\\nFailed Step: 6e7dc11a-b948-4ac1-b654-2fbdf1a594e6\\nError Message: ScriptExecutionException was caused by StreamAccessException.\\n  StreamAccessException was caused by NotFoundException.\\n    Found no resources for the input provided: 'https://evolveml3040211544.blob.core.windows.net/azureml-blobstore-899eb723-9471-475e-9ea2-abcad8a41de5/dataset/708ccaca-90d9-4a93-bb48-eadcab1cc401/transformed_data/'\\n| session_id=1d79f0f3-0532-417f-95b5-11e7caadde3f}, {'key': StackTrace, 'value':   File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/capability_session.py\\\", line 47, in start\\n    (data_path, sub_data_path) = session.start()\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\", line 182, in start\\n    if self._is_single_file:\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\", line 130, in _is_single_file\\n    path = dataflow._to_pyrecords()[0][temp_column]\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/dataflow.py\\\", line 758, in _to_pyrecords\\n    intermediate_files = _write_preppy_with_fallback('Dataflow.to_pyrecords', self, span_context=to_dprep_span_context(span.get_context()))\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 192, in _write_preppy_with_fallback\\n    _execute_with_fallback(activity, dataflow_to_execute, force_clex=force_clex, span_context=span_context)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 240, in _execute_with_fallback\\n    clex_execute()\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\", line 221, in clex_execute\\n    span_context=span_context\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_aml_helper.py\\\", line 38, in wrapper\\n    return send_message_func(op_code, message, cancellation_token)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/api.py\\\", line 159, in execute_anonymous_activity\\n    response = self._message_channel.send_message('Engine.ExecuteActivity', message_args, cancellation_token)\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/engine.py\\\", line 291, in send_message\\n    raise_engine_error(response['error'])\\n\\n  File \\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/errorhandlers.py\\\", line 10, in raise_engine_error\\n    raise ExecutionError(error_response)\\n}\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"{'code': data-capability.DatasetMountSession:input_7c0bc4da.ExecutionError, 'message': \\\\nError Code: ScriptExecution.StreamAccess.NotFound\\\\n, 'target': , 'category': UserError, 'error_details': [{'key': NonCompliantReason, 'value': \\\\nError Code: ScriptExecution.StreamAccess.NotFound\\\\nFailed Step: 6e7dc11a-b948-4ac1-b654-2fbdf1a594e6\\\\nError Message: ScriptExecutionException was caused by StreamAccessException.\\\\n  StreamAccessException was caused by NotFoundException.\\\\n    Found no resources for the input provided: 'https://evolveml3040211544.blob.core.windows.net/azureml-blobstore-899eb723-9471-475e-9ea2-abcad8a41de5/dataset/708ccaca-90d9-4a93-bb48-eadcab1cc401/transformed_data/'\\\\n| session_id=1d79f0f3-0532-417f-95b5-11e7caadde3f}, {'key': StackTrace, 'value':   File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/capability_session.py\\\\\\\", line 47, in start\\\\n    (data_path, sub_data_path) = session.start()\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\\\\\", line 182, in start\\\\n    if self._is_single_file:\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/data_capability/data_sessions.py\\\\\\\", line 130, in _is_single_file\\\\n    path = dataflow._to_pyrecords()[0][temp_column]\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/dataflow.py\\\\\\\", line 758, in _to_pyrecords\\\\n    intermediate_files = _write_preppy_with_fallback('Dataflow.to_pyrecords', self, span_context=to_dprep_span_context(span.get_context()))\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\\\\\", line 192, in _write_preppy_with_fallback\\\\n    _execute_with_fallback(activity, dataflow_to_execute, force_clex=force_clex, span_context=span_context)\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\\\\\", line 240, in _execute_with_fallback\\\\n    clex_execute()\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_dataframereader.py\\\\\\\", line 221, in clex_execute\\\\n    span_context=span_context\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/_aml_helper.py\\\\\\\", line 38, in wrapper\\\\n    return send_message_func(op_code, message, cancellation_token)\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/api.py\\\\\\\", line 159, in execute_anonymous_activity\\\\n    response = self._message_channel.send_message('Engine.ExecuteActivity', message_args, cancellation_token)\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/engineapi/engine.py\\\\\\\", line 291, in send_message\\\\n    raise_engine_error(response['error'])\\\\n\\\\n  File \\\\\\\"/opt/miniconda/envs/data-capability/lib/python3.7/site-packages/azureml/dataprep/api/errorhandlers.py\\\\\\\", line 10, in raise_engine_error\\\\n    raise ExecutionError(error_response)\\\\n}\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1650177288238
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineRun\n",
        "from azureml.core import Experiment\n",
        "\n",
        "# Publish the pipeline from the run\n",
        "submitted_pipeline_run = PipelineRun(experiment=Experiment(experiment, run_id=pipeline_run.id))\n",
        "published_pipeline = submitted_pipeline_run.publish_pipeline(name='data-drift-output-pipeline',\n",
        "    description='collect, transform and save datadrift output into dataset',\n",
        "    version='1.0',\n",
        "    continue_on_step_failure=False)\n",
        "\n",
        "print('Pipeline scheduled.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
        "\n",
        "# Schedules a daily run of a published pipeline\n",
        "daily = ScheduleRecurrence(frequency='Day', interval=1)\n",
        "pipeline_schedule = Schedule.create(ws, name='data_drift_output_schedule',\n",
        "                                        description='update data drift output every day',\n",
        "                                        pipeline_id=published_pipeline.id,\n",
        "                                        experiment_name='schedule_data_drift_output_pipeline',\n",
        "                                        recurrence=daily)\n",
        "\n",
        "print('Pipeline scheduled.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}